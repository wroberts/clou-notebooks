{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on Average Recommendation Scores\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.pyplot.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = [4.33, 4.33, 4, 4, 4, 3.67, 3.67, 3.67,\n",
    "          3.67, 3.50, 3.50, 3.50, 3.33, 3.33,\n",
    "          3.33, 3.33, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
    "          3, 3, 3, 3, 3, 2.67, 2.67, 2.67, 2.67, 2.67, 2.67,\n",
    "          2.67, 2.67, 2.67, 2.67, 2.50, 2.33, 2.33, 2.33, 2.33, 2.33, 2.33,\n",
    "          2.33, 2, 2, 2, 2, 2, 2, 2, 1.67, 1.67, 1.33,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.33: 1,\n",
       "         1.67: 2,\n",
       "         2: 7,\n",
       "         2.33: 7,\n",
       "         2.5: 1,\n",
       "         2.67: 10,\n",
       "         3: 14,\n",
       "         3.33: 4,\n",
       "         3.5: 3,\n",
       "         3.67: 4,\n",
       "         4: 3,\n",
       "         4.33: 2})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   2.,   7.,   8.,  10.,  14.,   4.,   7.,   3.,   2.]),\n",
       " array([ 1.33,  1.63,  1.93,  2.23,  2.53,  2.83,  3.13,  3.43,  3.73,\n",
       "         4.03,  4.33]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEECAYAAADNv0QiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcBglDwAGIySBwCDUfIAYDBKHgAM9v9Uz/mTMWtU8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e049ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.850690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.675823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.330000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  58.000000\n",
       "mean    2.850690\n",
       "std     0.675823\n",
       "min     1.330000\n",
       "25%     2.330000\n",
       "50%     3.000000\n",
       "75%     3.330000\n",
       "max     4.330000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    30\n",
       " dtype: int64, 0    16\n",
       " dtype: int64, 0    9\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df >=3).sum(), (df >=3.33).sum(), (df >=3.66).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentor Assignments\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Accepted</th>\n",
       "      <th>ID</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Title</th>\n",
       "      <th>Subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>9</td>\n",
       "      <td>Tomonori Kodaira, Tomoyuki Kajiwara and Mamoru...</td>\n",
       "      <td>Controlled and Balanced Dataset for Japanese L...</td>\n",
       "      <td>Resources and evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>10</td>\n",
       "      <td>Hitoshi Otsuki, Chenhui Chu, Toshiaki Nakazawa...</td>\n",
       "      <td>Dependency Forest based Word Alignment</td>\n",
       "      <td>Machine translation;  Tagging, chunking, synta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>18</td>\n",
       "      <td>Eric Benzschawel</td>\n",
       "      <td>Identifying Potential Adverse Drug Events in T...</td>\n",
       "      <td>Document analysis including text categorizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>19</td>\n",
       "      <td>Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto...</td>\n",
       "      <td>Generating Natural Language Descriptions for S...</td>\n",
       "      <td>Natural language generation;  Cognitive modeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>y(cv)</td>\n",
       "      <td>y</td>\n",
       "      <td>21</td>\n",
       "      <td>Alron Jan Lam</td>\n",
       "      <td>Improving Twitter Community Detection through ...</td>\n",
       "      <td>Sentiment analysis and opinion mining;  Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>24</td>\n",
       "      <td>Devadath V V and Dipti Misra Sharma</td>\n",
       "      <td>Significance of an Accurate Sandhi Splitter in...</td>\n",
       "      <td>Tagging, chunking, syntax, and parsing;  Phono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>27</td>\n",
       "      <td>Clare Llewellyn, Claire Grover and Jon Oberlander</td>\n",
       "      <td>Improving Topic Model Clustering of Newspaper ...</td>\n",
       "      <td>Social media;  Machine learning;  Summarizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>29</td>\n",
       "      <td>Taha Tobaili</td>\n",
       "      <td>Arabizi Detection in Twitter Data</td>\n",
       "      <td>Sentiment analysis and opinion mining;  Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>y(cv)</td>\n",
       "      <td>y</td>\n",
       "      <td>30</td>\n",
       "      <td>Dmitrijs Milajevs, Mehrnoosh Sadrzadeh and Mat...</td>\n",
       "      <td>Robust Co-occurrence Quantification for Lexica...</td>\n",
       "      <td>Semantics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>31</td>\n",
       "      <td>Hessel Haagsma</td>\n",
       "      <td>Singleton Detection using Word Embeddings and ...</td>\n",
       "      <td>Other;  Machine learning;  Semantics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>32</td>\n",
       "      <td>Murhaf Fares</td>\n",
       "      <td>A Dataset for Noun-Noun Compound Bracketing an...</td>\n",
       "      <td>Other;  Tagging, chunking, syntax, and parsing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>37</td>\n",
       "      <td>Omid Moradiannasab</td>\n",
       "      <td>Thesis Proposal: An Investigation on The Effec...</td>\n",
       "      <td>Document analysis including text categorizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>38</td>\n",
       "      <td>Vincent Kríž and Barbora Hladka</td>\n",
       "      <td>Improving Dependency Parsing Using Sentence Cl...</td>\n",
       "      <td>Tagging, chunking, syntax, and parsing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>39</td>\n",
       "      <td>Ewa Muszyńska</td>\n",
       "      <td>Graph- and surface-level sentence chunking</td>\n",
       "      <td>Tagging, chunking, syntax, and parsing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>40</td>\n",
       "      <td>Parth Mehta</td>\n",
       "      <td>From Extractive to Abstractive Summarization: ...</td>\n",
       "      <td>Document analysis including text categorizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>47</td>\n",
       "      <td>Gavin Abercrombie and Dirk Hovy</td>\n",
       "      <td>Putting Sarcasm Detection into Context: The Ef...</td>\n",
       "      <td>Social media;  Machine learning;  Sentiment an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>50</td>\n",
       "      <td>Alon Daks and Aidan Clark</td>\n",
       "      <td>Unsupervised Authorial Clustering Based on Syn...</td>\n",
       "      <td>Document analysis including text categorizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>y (cv)</td>\n",
       "      <td>y</td>\n",
       "      <td>52</td>\n",
       "      <td>Sapna Negi</td>\n",
       "      <td>Suggestion Mining from Opinionated Text</td>\n",
       "      <td>Sentiment analysis and opinion mining;  Social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>56</td>\n",
       "      <td>Yandi Xia and Yang Liu</td>\n",
       "      <td>An Efficient Cross-lingual Model for Sentence ...</td>\n",
       "      <td>Multilinguality;  Sentiment analysis and opini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>57</td>\n",
       "      <td>Timothy Lee, Alex Lutz and Jinho D. Choi</td>\n",
       "      <td>QA-It: Classifying Non-Referential It for Ques...</td>\n",
       "      <td>Discourse and pragmatics;  Resources and evalu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>58</td>\n",
       "      <td>Davaajav Jargalsaikhan, Naoaki Okazaki, Koji M...</td>\n",
       "      <td>Building a Corpus for Japanese Wikification wi...</td>\n",
       "      <td>Information extraction, text mining, and quest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>60</td>\n",
       "      <td>Vasu Jindal</td>\n",
       "      <td>A Personalized Markov Clustering and Deep Lear...</td>\n",
       "      <td>Document analysis including text categorizatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Accepted  ID                                            Authors  \\\n",
       "0        y        y   9  Tomonori Kodaira, Tomoyuki Kajiwara and Mamoru...   \n",
       "1        y        y  10  Hitoshi Otsuki, Chenhui Chu, Toshiaki Nakazawa...   \n",
       "2        y        y  18                                   Eric Benzschawel   \n",
       "3        y        y  19  Eri Matsuo, Ichiro Kobayashi, Shinji Nishimoto...   \n",
       "4    y(cv)        y  21                                      Alron Jan Lam   \n",
       "5        y        y  24                Devadath V V and Dipti Misra Sharma   \n",
       "6        y        y  27  Clare Llewellyn, Claire Grover and Jon Oberlander   \n",
       "7        y        y  29                                       Taha Tobaili   \n",
       "8    y(cv)        y  30  Dmitrijs Milajevs, Mehrnoosh Sadrzadeh and Mat...   \n",
       "9        y        y  31                                     Hessel Haagsma   \n",
       "10       y        y  32                                       Murhaf Fares   \n",
       "11       y        y  37                                 Omid Moradiannasab   \n",
       "12       y        y  38                    Vincent Kríž and Barbora Hladka   \n",
       "13       y        y  39                                      Ewa Muszyńska   \n",
       "14       y        y  40                                        Parth Mehta   \n",
       "15       y        y  47                    Gavin Abercrombie and Dirk Hovy   \n",
       "16       y        y  50                          Alon Daks and Aidan Clark   \n",
       "17  y (cv)        y  52                                         Sapna Negi   \n",
       "18       y        y  56                             Yandi Xia and Yang Liu   \n",
       "19       y        y  57           Timothy Lee, Alex Lutz and Jinho D. Choi   \n",
       "20       y        y  58  Davaajav Jargalsaikhan, Naoaki Okazaki, Koji M...   \n",
       "21       y        y  60                                        Vasu Jindal   \n",
       "\n",
       "                                                Title  \\\n",
       "0   Controlled and Balanced Dataset for Japanese L...   \n",
       "1              Dependency Forest based Word Alignment   \n",
       "2   Identifying Potential Adverse Drug Events in T...   \n",
       "3   Generating Natural Language Descriptions for S...   \n",
       "4   Improving Twitter Community Detection through ...   \n",
       "5   Significance of an Accurate Sandhi Splitter in...   \n",
       "6   Improving Topic Model Clustering of Newspaper ...   \n",
       "7                   Arabizi Detection in Twitter Data   \n",
       "8   Robust Co-occurrence Quantification for Lexica...   \n",
       "9   Singleton Detection using Word Embeddings and ...   \n",
       "10  A Dataset for Noun-Noun Compound Bracketing an...   \n",
       "11  Thesis Proposal: An Investigation on The Effec...   \n",
       "12  Improving Dependency Parsing Using Sentence Cl...   \n",
       "13         Graph- and surface-level sentence chunking   \n",
       "14  From Extractive to Abstractive Summarization: ...   \n",
       "15  Putting Sarcasm Detection into Context: The Ef...   \n",
       "16  Unsupervised Authorial Clustering Based on Syn...   \n",
       "17            Suggestion Mining from Opinionated Text   \n",
       "18  An Efficient Cross-lingual Model for Sentence ...   \n",
       "19  QA-It: Classifying Non-Referential It for Ques...   \n",
       "20  Building a Corpus for Japanese Wikification wi...   \n",
       "21  A Personalized Markov Clustering and Deep Lear...   \n",
       "\n",
       "                                             Subjects  \n",
       "0                            Resources and evaluation  \n",
       "1   Machine translation;  Tagging, chunking, synta...  \n",
       "2   Document analysis including text categorizatio...  \n",
       "3   Natural language generation;  Cognitive modeli...  \n",
       "4   Sentiment analysis and opinion mining;  Social...  \n",
       "5   Tagging, chunking, syntax, and parsing;  Phono...  \n",
       "6   Social media;  Machine learning;  Summarizatio...  \n",
       "7   Sentiment analysis and opinion mining;  Docume...  \n",
       "8                                           Semantics  \n",
       "9                Other;  Machine learning;  Semantics  \n",
       "10  Other;  Tagging, chunking, syntax, and parsing...  \n",
       "11  Document analysis including text categorizatio...  \n",
       "12             Tagging, chunking, syntax, and parsing  \n",
       "13             Tagging, chunking, syntax, and parsing  \n",
       "14  Document analysis including text categorizatio...  \n",
       "15  Social media;  Machine learning;  Sentiment an...  \n",
       "16  Document analysis including text categorizatio...  \n",
       "17  Sentiment analysis and opinion mining;  Social...  \n",
       "18  Multilinguality;  Sentiment analysis and opini...  \n",
       "19  Discourse and pragmatics;  Resources and evalu...  \n",
       "20  Information extraction, text mining, and quest...  \n",
       "21  Document analysis including text categorizatio...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAPERS = '''\n",
    "y\ty\t9\tTomonori Kodaira, Tomoyuki Kajiwara and Mamoru Komachi\tControlled and Balanced Dataset for Japanese Lexical Simplification\tResources and evaluation\n",
    "y\ty\t10\tHitoshi Otsuki, Chenhui Chu, Toshiaki Nakazawa and Sadao Kurohashi\tDependency Forest based Word Alignment\tMachine translation;  Tagging, chunking, syntax, and parsing\n",
    "y\ty\t18\tEric Benzschawel\tIdentifying Potential Adverse Drug Events in Tweets Using Bootstrapped Lexicons\tDocument analysis including text categorization, topic models, and retrieval;  Social media;  Information extraction, text mining, and question answering;  Resources and evaluation\n",
    "y\ty\t19\tEri Matsuo, Ichiro Kobayashi, Shinji Nishimoto, Satoshi Nishida and Hideki Asoh\tGenerating Natural Language Descriptions for Semantic Representations of Human Brain Activity\tNatural language generation;  Cognitive modeling and psycholinguistics;  Vision, robots, and other grounding\n",
    "y(cv)\ty\t21\tAlron Jan Lam\tImproving Twitter Community Detection through Contextual Sentiment Analysis\tSentiment analysis and opinion mining;  Social media;  Discourse and pragmatics\n",
    "y\ty\t24\tDevadath V V and Dipti Misra Sharma\tSignificance of an Accurate Sandhi Splitter in Shallow Parsing of Dravidian languages\tTagging, chunking, syntax, and parsing;  Phonology, morphology, and word segmentation;  Resources and evaluation\n",
    "y\ty\t27\tClare Llewellyn, Claire Grover and Jon Oberlander\tImproving Topic Model Clustering of Newspaper Comments for Summarisation\tSocial media;  Machine learning;  Summarization;  Document analysis including text categorization, topic models, and retrieval\n",
    "y\ty\t29\tTaha Tobaili\tArabizi Detection in Twitter Data\tSentiment analysis and opinion mining;  Document analysis including text categorization, topic models, and retrieval;  Social media;  Machine learning;  Multilinguality\n",
    "y(cv)\ty\t30\tDmitrijs Milajevs, Mehrnoosh Sadrzadeh and Matthew Purver\tRobust Co-occurrence Quantification for Lexical Distributional Semantics\tSemantics\n",
    "y\ty\t31\tHessel Haagsma\tSingleton Detection using Word Embeddings and Neural Networks\tOther;  Machine learning;  Semantics\n",
    "y\ty\t32\tMurhaf Fares\tA Dataset for Noun-Noun Compound Bracketing and Interpretation\tOther;  Tagging, chunking, syntax, and parsing;  Resources and evaluation;  Semantics\n",
    "y\ty\t37\tOmid Moradiannasab\tThesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents\tDocument analysis including text categorization, topic models, and retrieval;  Dialog and interactive systems;  Natural language generation;  Vision, robots, and other grounding;  Cognitive modeling and psycholinguistics\n",
    "y\ty\t38\tVincent Kríž and Barbora Hladka\tImproving Dependency Parsing Using Sentence Clause Charts\tTagging, chunking, syntax, and parsing\n",
    "y\ty\t39\tEwa Muszyńska\tGraph- and surface-level sentence chunking\tTagging, chunking, syntax, and parsing\n",
    "y\ty\t40\tParth Mehta\tFrom Extractive to Abstractive Summarization: A Journey\tDocument analysis including text categorization, topic models, and retrieval;  Natural language generation;  Summarization\n",
    "y\ty\t47\tGavin Abercrombie and Dirk Hovy\tPutting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations\tSocial media;  Machine learning;  Sentiment analysis and opinion mining;  Document analysis including text categorization, topic models, and retrieval\n",
    "y\ty\t50\tAlon Daks and Aidan Clark\tUnsupervised Authorial Clustering Based on Syntactic Structure\tDocument analysis including text categorization, topic models, and retrieval\n",
    "y (cv)\ty\t52\tSapna Negi\tSuggestion Mining from Opinionated Text\tSentiment analysis and opinion mining;  Social media\n",
    "y\ty\t56\tYandi Xia and Yang Liu\tAn Efficient Cross-lingual Model for Sentence Classification Without Using Machine Translation\tMultilinguality;  Sentiment analysis and opinion mining;  Document analysis including text categorization, topic models, and retrieval;  Information extraction, text mining, and question answering\n",
    "y\ty\t57\tTimothy Lee, Alex Lutz and Jinho D. Choi\tQA-It: Classifying Non-Referential It for Question Answer Pairs\tDiscourse and pragmatics;  Resources and evaluation\n",
    "y\ty\t58\tDavaajav Jargalsaikhan, Naoaki Okazaki, Koji Matsuda and Kentaro Inui\tBuilding a Corpus for Japanese Wikification with Fine-Grained Entity Classes\tInformation extraction, text mining, and question answering\n",
    "y\ty\t60\tVasu Jindal\tA Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization\tDocument analysis including text categorization, topic models, and retrieval;  Machine learning;  Semantics\u0011\n",
    "'''\n",
    "PAPERS = pandas.DataFrame([x.split('\\t') for x in PAPERS.strip().split('\\n')])\n",
    "PAPERS.columns = ['Type','Accepted','ID','Authors','Title','Subjects']\n",
    "ALL_TAGS = reduce(set.union, [set(subj.strip() for subj in tag.split(';')) for tag in PAPERS.Subjects])\n",
    "PAPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ann.copestake': {'Semantics'},\n",
       " 'bishan': {'Discourse and pragmatics',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Sentiment analysis and opinion mining'},\n",
       " 'bonnie': {'Discourse and pragmatics',\n",
       "  'Machine translation',\n",
       "  'Resources and evaluation'},\n",
       " 'cbrew': {'Dialog and interactive systems',\n",
       "  'Discourse and pragmatics',\n",
       "  'Machine translation',\n",
       "  'Multilinguality',\n",
       "  'Phonology, morphology, and word segmentation',\n",
       "  'Resources and evaluation',\n",
       "  'Semantics',\n",
       "  'Summarization',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'cdq10131': {'Information extraction, text mining, and question answering',\n",
       "  'Machine learning',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'chenhao': {'Discourse and pragmatics', 'Social media'},\n",
       " 'christophteichmann': {'Machine learning',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'dyogatama': {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Machine learning'},\n",
       " 'epitler': {'Tagging, chunking, syntax, and parsing'},\n",
       " 'gneubig': {'Machine learning',\n",
       "  'Machine translation',\n",
       "  'Multilinguality',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'gpenn': {'Dialog and interactive systems',\n",
       "  'Phonology, morphology, and word segmentation',\n",
       "  'Semantics',\n",
       "  'Summarization',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'hovy': {'Discourse and pragmatics',\n",
       "  'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Resources and evaluation',\n",
       "  'Semantics',\n",
       "  'Sentiment analysis and opinion mining',\n",
       "  'Summarization'},\n",
       " 'karthikn': {'Information extraction, text mining, and question answering',\n",
       "  'Semantics'},\n",
       " 'ko': {'Machine translation',\n",
       "  'Phonology, morphology, and word segmentation',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'li.jessy': {'Discourse and pragmatics', 'Multilinguality', 'Summarization'},\n",
       " 'microth': {'Semantics'},\n",
       " 'moschitti': {'Dialog and interactive systems',\n",
       "  'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Machine learning',\n",
       "  'Semantics',\n",
       "  'Sentiment analysis and opinion mining',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'nivre': {'Machine learning',\n",
       "  'Multilinguality',\n",
       "  'Resources and evaluation',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 'sanda': {'Information extraction, text mining, and question answering'},\n",
       " 'tsmoon': {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Machine learning',\n",
       "  'Phonology, morphology, and word segmentation',\n",
       "  'Resources and evaluation',\n",
       "  'Semantics',\n",
       "  'Summarization',\n",
       "  'Tagging, chunking, syntax, and parsing'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MENTORS = '''\n",
    "moschitti\tDialog and interactive systems, Document analysis including text categorization, topic models, and retrieval, Information extraction, text mining, and question answering, Machine learning, Semantics, Sentiment analysis and opinion mining, Tagging, chunking, syntax, and parsing\n",
    "ann.copestake\tGeneration, Semantics\n",
    "bishan\tDiscourse and pragmatics, Information extraction, text mining, and question answering, Sentiment analysis and opinion mining\n",
    "bonnie OR bonnie.webber\tDiscourse and pragmatics, Machine translation, Resources and evaluation\n",
    "chenhao\tDiscourse and pragmatics, Social media\n",
    "christophteichmann\tMachine learning, Tagging, chunking, syntax, and parsing\n",
    "cdq10131\tInformation extraction, text mining, and question answering, Machine learning, Tagging, chunking, syntax, and parsing\n",
    "hovy\tDiscourse and pragmatics, Document analysis including text categorization, topic models, and retrieval, Generation, Information extraction, text mining, and question answering, Resources and evaluation, Semantics, Sentiment analysis and opinion mining, Summarization\n",
    "epitler\tTagging, chunking, syntax, and parsing\n",
    "gpenn\tDialog and interactive systems, Phonology, morphology, and word segmentation, Semantics, Speech, Summarization, Tagging, chunking, syntax, and parsing\n",
    "nivre\tMachine learning, Multilinguality, Resources and evaluation, Tagging, chunking, syntax, and parsing\n",
    "li.jessy\tDiscourse and pragmatics, Multilinguality, Summarization\n",
    "karthikn\tInformation extraction, text mining, and question answering, Semantics\n",
    "ko\tMachine translation, Phonology, morphology, and word segmentation, Tagging, chunking, syntax, and parsing\n",
    "microth\tSemantics\n",
    "sanda\tInformation extraction, text mining, and question answering\n",
    "cbrew\tCognitive modeling of language processing and psycholinguistics, Dialog and interactive systems, Discourse and pragmatics, Generation, Machine translation, Multilinguality, Phonology, morphology, and word segmentation, Resources and evaluation, Semantics, Speech, Summarization, Tagging, chunking, syntax, and parsing\n",
    "dyogatama\tDocument analysis including text categorization, topic models, and retrieval, Machine learning\n",
    "tsmoon\tDocument analysis including text categorization, topic models, and retrieval, Information extraction, text mining, and question answering, Machine learning, Phonology, morphology, and word segmentation, Resources and evaluation, Semantics, Summarization, Tagging, chunking, syntax, and parsing\n",
    "gneubig\tMachine learning, Machine translation, Multilinguality, Tagging, chunking, syntax, and parsing\n",
    "'''\n",
    "\n",
    "MENTORS = [x.split('\\t') for x in MENTORS.strip().split('\\n')]\n",
    "MENTORS = dict((x,set(s for s in ALL_TAGS if s in y)) for (x,y) in MENTORS)\n",
    "MENTORS['bonnie'] = MENTORS['bonnie OR bonnie.webber']\n",
    "del MENTORS['bonnie OR bonnie.webber']\n",
    "MENTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: {'Resources and evaluation'},\n",
       " 10: {'Machine translation', 'Tagging, chunking, syntax, and parsing'},\n",
       " 18: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Resources and evaluation',\n",
       "  'Social media'},\n",
       " 19: {'Cognitive modeling and psycholinguistics',\n",
       "  'Natural language generation',\n",
       "  'Vision, robots, and other grounding'},\n",
       " 21: {'Discourse and pragmatics',\n",
       "  'Sentiment analysis and opinion mining',\n",
       "  'Social media'},\n",
       " 24: {'Phonology, morphology, and word segmentation',\n",
       "  'Resources and evaluation',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 27: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Machine learning',\n",
       "  'Social media',\n",
       "  'Summarization'},\n",
       " 29: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Machine learning',\n",
       "  'Multilinguality',\n",
       "  'Sentiment analysis and opinion mining',\n",
       "  'Social media'},\n",
       " 30: {'Semantics'},\n",
       " 31: {'Machine learning', 'Other', 'Semantics'},\n",
       " 32: {'Other',\n",
       "  'Resources and evaluation',\n",
       "  'Semantics',\n",
       "  'Tagging, chunking, syntax, and parsing'},\n",
       " 37: {'Cognitive modeling and psycholinguistics',\n",
       "  'Dialog and interactive systems',\n",
       "  'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Natural language generation',\n",
       "  'Vision, robots, and other grounding'},\n",
       " 38: {'Tagging, chunking, syntax, and parsing'},\n",
       " 39: {'Tagging, chunking, syntax, and parsing'},\n",
       " 40: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Natural language generation',\n",
       "  'Summarization'},\n",
       " 47: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Machine learning',\n",
       "  'Sentiment analysis and opinion mining',\n",
       "  'Social media'},\n",
       " 50: {'Document analysis including text categorization, topic models, and retrieval'},\n",
       " 52: {'Sentiment analysis and opinion mining', 'Social media'},\n",
       " 56: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Information extraction, text mining, and question answering',\n",
       "  'Multilinguality',\n",
       "  'Sentiment analysis and opinion mining'},\n",
       " 57: {'Discourse and pragmatics', 'Resources and evaluation'},\n",
       " 58: {'Information extraction, text mining, and question answering'},\n",
       " 60: {'Document analysis including text categorization, topic models, and retrieval',\n",
       "  'Machine learning',\n",
       "  'Semantics\\x11'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_subjs = dict(zip(map(int,PAPERS.ID), [set(y.strip() for y in x.split(';')) for x in PAPERS.Subjects]))\n",
    "paper_subjs\n",
    "#len(paper_subjs) is 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ann.copestake': {30, 31, 32},\n",
       " 'bishan': {18, 21, 29, 47, 52, 56, 57, 58},\n",
       " 'bonnie': {9, 10, 18, 21, 24, 32, 57},\n",
       " 'cbrew': {9, 10, 18, 21, 24, 27, 29, 30, 31, 32, 37, 38, 39, 40, 56, 57},\n",
       " 'cdq10131': {10, 18, 24, 27, 29, 31, 32, 38, 39, 47, 56, 58, 60},\n",
       " 'chenhao': {18, 21, 27, 29, 47, 52, 57},\n",
       " 'christophteichmann': {10, 24, 27, 29, 31, 32, 38, 39, 47, 60},\n",
       " 'dyogatama': {18, 27, 29, 31, 37, 40, 47, 50, 56, 60},\n",
       " 'epitler': {10, 24, 32, 38, 39},\n",
       " 'gneubig': {10, 24, 27, 29, 31, 32, 38, 39, 47, 56, 60},\n",
       " 'gpenn': {10, 24, 27, 30, 31, 32, 37, 38, 39, 40},\n",
       " 'hovy': {9,\n",
       "  18,\n",
       "  21,\n",
       "  24,\n",
       "  27,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  37,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  52,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  60},\n",
       " 'karthikn': {18, 30, 31, 32, 56, 58},\n",
       " 'ko': {10, 24, 32, 38, 39},\n",
       " 'li.jessy': {21, 27, 29, 40, 56, 57},\n",
       " 'microth': {30, 31, 32},\n",
       " 'moschitti': {10,\n",
       "  18,\n",
       "  21,\n",
       "  24,\n",
       "  27,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  52,\n",
       "  56,\n",
       "  58,\n",
       "  60},\n",
       " 'nivre': {9, 10, 18, 24, 27, 29, 31, 32, 38, 39, 47, 56, 57, 60},\n",
       " 'sanda': {18, 56, 58},\n",
       " 'tsmoon': {9,\n",
       "  10,\n",
       "  18,\n",
       "  24,\n",
       "  27,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  60}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibles = dict((reviewer, set(paper_id for paper_id, subjs in paper_subjs.items() if subjs & rsubjs)) for (reviewer, rsubjs) in MENTORS.items())\n",
    "#possibles['bonnie'] = possibles['bonnie OR bonnie.webber']\n",
    "#del possibles['bonnie OR bonnie.webber']\n",
    "possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'ahb', 'habash', 'rws'},\n",
       " 2: {'avgustinova', 'chenhao', 'jiangfeng1124'},\n",
       " 3: {'elhadad', 'marcu', 'vince'},\n",
       " 4: {'a.zarcone', 'jiangfeng1124', 'microth'},\n",
       " 5: {'eric.laporte', 'gpenn', 'nivre'},\n",
       " 6: {'chenhao', 'gneubig', 'jiangfeng1124'},\n",
       " 7: {'a.zarcone', 'bonnie', 'sekine'},\n",
       " 9: {'gneubig', 'riley', 'simone.teufel'},\n",
       " 10: {'cdq10131', 'epitler', 'riley'},\n",
       " 11: {'eric.laporte', 'jiangfeng1124', 'tsmoon'},\n",
       " 12: {'epitler', 'foster', 'marcu'},\n",
       " 13: {'kysu', 'moschitti', 'npeng'},\n",
       " 14: {'frcchang', 'nivre', 'zero12'},\n",
       " 15: {'a.zarcone', 'miles', 'wanxiang'},\n",
       " 16: {'hovy', 'kysu', 'luwei'},\n",
       " 17: {'bishan', 'vince', 'yannick.parmentier'},\n",
       " 18: {'kgimpel', 'marcu', 'miles'},\n",
       " 19: {'foster', 'karthikn', 'wanxiang'},\n",
       " 20: {'bishan', 'tsmoon', 'wanxiang'},\n",
       " 21: {'bishan', 'eric.laporte', 'masonzms'},\n",
       " 22: {'ko', 'kysu', 'sanda'},\n",
       " 23: {'bonnie', 'david.schlangen', 'xiaohe'},\n",
       " 24: {'larshellan', 'nivre', 'rws'},\n",
       " 25: {'dyogatama', 'sekine', 'vince'},\n",
       " 26: {'ann.copestake', 'cdq10131', 'dyogatama'},\n",
       " 27: {'cbrew', 'elhadad', 'simone.teufel'},\n",
       " 28: {'ann.copestake', 'kgimpel', 'luwei'},\n",
       " 29: {'dyogatama', 'miles', 'vince'},\n",
       " 30: {'david.schlangen', 'hovy', 'larshellan'},\n",
       " 31: {'angeli', 'epitler', 'npeng'},\n",
       " 32: {'christophteichmann', 'epitler', 'rws'},\n",
       " 34: {'angeli', 'moschitti', 'sekine'},\n",
       " 35: {'kbhall', 'riley', 'tsmoon'},\n",
       " 36: {'foster', 'gneubig', 'pkoehn'},\n",
       " 37: {'cbrew', 'gpenn', 'karthikn'},\n",
       " 38: {'cdq10131', 'nivre', 'yuanzh'},\n",
       " 39: {'elhadad', 'ko', 'zero12'},\n",
       " 40: {'gpenn', 'li.jessy', 'masonzms'},\n",
       " 41: {'ko', 'rws', 'zero12'},\n",
       " 42: {'cdq10131', 'david.schlangen', 'kbhall'},\n",
       " 43: {'david.schlangen', 'dilekha', 'riley'},\n",
       " 44: {'dilekha', 'microth', 'npeng'},\n",
       " 45: {'bdlijiwei', 'miles', 'wangwilliamyang'},\n",
       " 46: {'bonnie', 'kgimpel', 'wangwilliamyang'},\n",
       " 47: {'ann.copestake', 'bdlijiwei', 'masonzms'},\n",
       " 48: {'hovy', 'kgimpel', 'simone.teufel'},\n",
       " 49: {'christophteichmann', 'sanda', 'wangwilliamyang'},\n",
       " 50: {'ahb', 'ann.copestake', 'yannick.parmentier'},\n",
       " 51: {'a.zarcone', 'jamesp', 'microth'},\n",
       " 52: {'bdlijiwei', 'christophteichmann', 'li.jessy'},\n",
       " 53: {'frcchang', 'masonzms', 'yuanzh'},\n",
       " 54: {'kysu', 'wangwilliamyang', 'wanxiang'},\n",
       " 56: {'cbrew', 'christophteichmann', 'larshellan'},\n",
       " 57: {'elhadad', 'li.jessy', 'moschitti'},\n",
       " 58: {'gneubig', 'sanda', 'sekine'},\n",
       " 59: {'avgustinova', 'foster', 'pkoehn'},\n",
       " 60: {'habash', 'npeng', 'xiaohe'},\n",
       " 61: {'eric.laporte', 'habash', 'jamesp'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RASSTS = '''\n",
    "9:gneubig:riley:simone.teufel:\n",
    "10:cdq10131:epitler:riley:\n",
    "18:kgimpel:marcu:miles:\n",
    "19:foster:karthikn:wanxiang:\n",
    "21:bishan:eric.laporte:masonzms:\n",
    "24:larshellan:nivre:rws:\n",
    "27:cbrew:elhadad:simone.teufel:\n",
    "29:dyogatama:miles:vince:\n",
    "30:david.schlangen:hovy:larshellan:\n",
    "31:angeli:epitler:npeng:\n",
    "32:christophteichmann:epitler:rws:\n",
    "37:cbrew:gpenn:karthikn:\n",
    "38:cdq10131:nivre:yuanzh:\n",
    "39:elhadad:ko:zero12:\n",
    "40:gpenn:li.jessy:masonzms:\n",
    "47:ann.copestake:bdlijiwei:masonzms:\n",
    "50:ahb:ann.copestake:yannick.parmentier:\n",
    "52:bdlijiwei:christophteichmann:li.jessy:\n",
    "56:cbrew:christophteichmann:larshellan:\n",
    "57:elhadad:li.jessy:moschitti:\n",
    "58:gneubig:sanda:sekine:\n",
    "60:habash:npeng:xiaohe:\n",
    "7:a.zarcone:bonnie:sekine:\n",
    "26:ann.copestake:cdq10131:dyogatama:\n",
    "17:bishan:vince:yannick.parmentier:\n",
    "2:avgustinova:chenhao:jiangfeng1124:\n",
    "1:ahb:habash:rws:\n",
    "16:hovy:kysu:luwei:\n",
    "44:dilekha:microth:npeng:\n",
    "25:dyogatama:sekine:vince:\n",
    "28:ann.copestake:kgimpel:luwei:\n",
    "61:eric.laporte:habash:jamesp:\n",
    "14:frcchang:nivre:zero12:\n",
    "20:bishan:tsmoon:wanxiang:\n",
    "59:avgustinova:foster:pkoehn:\n",
    "49:christophteichmann:sanda:wangwilliamyang:\n",
    "35:kbhall:riley:tsmoon:\n",
    "11:eric.laporte:jiangfeng1124:tsmoon:\n",
    "53:frcchang:masonzms:yuanzh:\n",
    "48:hovy:kgimpel:simone.teufel:\n",
    "22:ko:kysu:sanda:\n",
    "42:cdq10131:david.schlangen:kbhall:\n",
    "46:bonnie:kgimpel:wangwilliamyang:\n",
    "23:bonnie:david.schlangen:xiaohe:\n",
    "13:kysu:moschitti:npeng:\n",
    "6:chenhao:gneubig:jiangfeng1124:\n",
    "3:elhadad:marcu:vince:\n",
    "36:foster:gneubig:pkoehn:\n",
    "51:a.zarcone:jamesp:microth:\n",
    "41:ko:rws:zero12:\n",
    "12:epitler:foster:marcu:\n",
    "15:a.zarcone:miles:wanxiang:\n",
    "4:a.zarcone:jiangfeng1124:microth:\n",
    "34:angeli:moschitti:sekine:\n",
    "45:bdlijiwei:miles:wangwilliamyang:\n",
    "43:david.schlangen:dilekha:riley:\n",
    "54:kysu:wangwilliamyang:wanxiang:\n",
    "5:eric.laporte:gpenn:nivre:\n",
    "'''\n",
    "RASSTS = [x.split(':') for x in RASSTS.strip().split('\\n')]\n",
    "RASSTS = dict((int(fs[0]), set(fs[1:4])) for fs in RASSTS)\n",
    "RASSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ann.copestake': {30, 31, 32},\n",
       " 'bishan': {18, 29, 47, 52, 56, 57, 58},\n",
       " 'bonnie': {9, 10, 18, 21, 24, 32, 57},\n",
       " 'cbrew': {9, 10, 18, 21, 24, 29, 30, 31, 32, 38, 39, 40, 57},\n",
       " 'cdq10131': {18, 24, 27, 29, 31, 32, 39, 47, 56, 58, 60},\n",
       " 'chenhao': {18, 21, 27, 29, 47, 52, 57},\n",
       " 'christophteichmann': {10, 24, 27, 29, 31, 38, 39, 47, 60},\n",
       " 'dyogatama': {18, 27, 31, 37, 40, 47, 50, 56, 60},\n",
       " 'epitler': {24, 38, 39},\n",
       " 'gneubig': {10, 24, 27, 29, 31, 32, 38, 39, 47, 56, 60},\n",
       " 'gpenn': {10, 24, 27, 30, 31, 32, 38, 39},\n",
       " 'hovy': {9, 18, 21, 24, 27, 29, 31, 32, 37, 40, 47, 50, 52, 56, 57, 58, 60},\n",
       " 'karthikn': {18, 30, 31, 32, 56, 58},\n",
       " 'ko': {10, 24, 32, 38},\n",
       " 'li.jessy': {21, 27, 29, 56},\n",
       " 'microth': {30, 31, 32},\n",
       " 'moschitti': {10,\n",
       "  18,\n",
       "  21,\n",
       "  24,\n",
       "  27,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  52,\n",
       "  56,\n",
       "  58,\n",
       "  60},\n",
       " 'nivre': {9, 10, 18, 27, 29, 31, 32, 39, 47, 56, 57, 60},\n",
       " 'sanda': {18, 56},\n",
       " 'tsmoon': {9,\n",
       "  10,\n",
       "  18,\n",
       "  24,\n",
       "  27,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  47,\n",
       "  50,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  60}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (aid, revrs) in RASSTS.items():\n",
    "    for revr in revrs:\n",
    "        if revr in possibles:\n",
    "            possibles[revr].discard(aid)\n",
    "possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10),\n",
       " (1, 20),\n",
       " (2, 19),\n",
       " (3, 14),\n",
       " (4, 21),\n",
       " (5, 17),\n",
       " (6, 15),\n",
       " (7, 18),\n",
       " (8, 13),\n",
       " (9, 12),\n",
       " (10, 6),\n",
       " (11, 16),\n",
       " (12, 9),\n",
       " (13, 5),\n",
       " (14, 7),\n",
       " (15, 8),\n",
       " (16, 11),\n",
       " (17, 1),\n",
       " (17, 4),\n",
       " (18, 2),\n",
       " (19, 0),\n",
       " (19, 3)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycosat\n",
    "len(PAPERS),len(possibles)\n",
    "pids = map(int,PAPERS.ID)\n",
    "NUM_PAPERS = len(pids)\n",
    "NUM_MENTORS = len(possibles)\n",
    "# papers are 0 to 21, reviewers are 0 to 19 incl.\n",
    "# reviewer X has paper Y : 22 * Y + X + 1\n",
    "def clauseid(reviewer, paper): return paper + NUM_PAPERS * reviewer + 1\n",
    "\n",
    "def unclauseid(cid): return (cid - 1) // NUM_PAPERS, (cid - 1) % NUM_PAPERS\n",
    "\n",
    "clauses = []\n",
    "\n",
    "# each paper must be mentored by somebody\n",
    "#for p in PAPERS.ID:\n",
    "#    clauses.append([clauseid(r,int(p)-1) for r in range(22)])\n",
    "for p in range(NUM_PAPERS):\n",
    "    clauses.append([clauseid(r,p) for r in range(NUM_MENTORS)])\n",
    "        \n",
    "# each paper can only be mentored by one person\n",
    "for p in range(NUM_PAPERS):\n",
    "    for r in range(NUM_MENTORS):\n",
    "        for r2 in range(NUM_MENTORS):\n",
    "            if r == r2:\n",
    "                continue\n",
    "            clauses.append([-clauseid(r,p), -clauseid(r2,p)])\n",
    "            \n",
    "# every reviewer mentors at least one paper\n",
    "for r in range(NUM_MENTORS):\n",
    "    clauses.append([clauseid(r,p) for p in range(NUM_PAPERS)])\n",
    "            \n",
    "# reviewers can only mentor certain papers\n",
    "rids = sorted(possibles.keys())\n",
    "for r, rid in enumerate(rids):\n",
    "    clauses.append([clauseid(r,pids.index(x)) for x in possibles[rid]])\n",
    "    for pid in set(range(NUM_PAPERS)) - set(pids.index(x) for x in possibles[rid]):\n",
    "        pass #clauses.append([-clauseid(r,pid)])\n",
    "        \n",
    "# no reviewer mentors more than two papers\n",
    "for r in range(NUM_MENTORS):\n",
    "    for p1 in range(NUM_PAPERS):\n",
    "        for p2 in range(NUM_PAPERS):\n",
    "            if p1 == p2:\n",
    "                continue\n",
    "            for p3 in range(NUM_PAPERS):\n",
    "                if p1 == p3 or p2 == p3:\n",
    "                    continue\n",
    "                clauses.append([-clauseid(r,p1),-clauseid(r,p2),-clauseid(r,p3)])\n",
    "                \n",
    "# gpenn and sanda: only one paper\n",
    "for p1 in range(NUM_PAPERS):\n",
    "    for p2 in range(NUM_PAPERS):\n",
    "        if p1 == p2:\n",
    "            continue\n",
    "        clauses.append([-clauseid(rids.index('gpenn'),p1),-clauseid(rids.index('gpenn'),p2)])\n",
    "        clauses.append([-clauseid(rids.index('sanda'),p1),-clauseid(rids.index('sanda'),p2)])\n",
    "    \n",
    "soln = pycosat.solve(clauses)\n",
    "if soln == 'UNSAT':\n",
    "    print soln\n",
    "soln = [x for x in soln if x > 0]\n",
    "[unclauseid(x) for x in soln]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194146"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 22)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_soln(soln):\n",
    "    score = 0\n",
    "    for cnum in soln:\n",
    "        if cnum <= 0:\n",
    "            continue\n",
    "        ridx, pidx = unclauseid(cnum)\n",
    "        rid = rids[ridx]\n",
    "        pid = pids[pidx]\n",
    "        score += len(paper_subjs[pid] & MENTORS[rid])\n",
    "    return score\n",
    "len(rids),len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "New best score 23\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 58\n",
      "bonnie mentors 57\n",
      "cbrew mentors 40\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 47\n",
      "dyogatama mentors 56\n",
      "epitler mentors 39\n",
      "gneubig mentors 38\n",
      "gpenn mentors 27\n",
      "hovy mentors 50\n",
      "karthikn mentors 31\n",
      "ko mentors 24\n",
      "li.jessy mentors 29\n",
      "microth mentors 30\n",
      "moschitti mentors 37\n",
      "nivre mentors 10\n",
      "nivre mentors 21\n",
      "sanda mentors 18\n",
      "tsmoon mentors 9\n",
      "tsmoon mentors 19\n",
      "---\n",
      "New best score 24\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 58\n",
      "bonnie mentors 57\n",
      "cbrew mentors 40\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 47\n",
      "dyogatama mentors 56\n",
      "epitler mentors 39\n",
      "gneubig mentors 38\n",
      "gpenn mentors 27\n",
      "hovy mentors 50\n",
      "karthikn mentors 31\n",
      "ko mentors 24\n",
      "li.jessy mentors 29\n",
      "microth mentors 30\n",
      "moschitti mentors 21\n",
      "moschitti mentors 37\n",
      "nivre mentors 9\n",
      "sanda mentors 18\n",
      "tsmoon mentors 10\n",
      "tsmoon mentors 19\n",
      "---\n",
      "New best score 26\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 58\n",
      "bonnie mentors 57\n",
      "cbrew mentors 40\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 47\n",
      "dyogatama mentors 56\n",
      "epitler mentors 39\n",
      "gneubig mentors 38\n",
      "gpenn mentors 27\n",
      "hovy mentors 50\n",
      "karthikn mentors 31\n",
      "ko mentors 24\n",
      "li.jessy mentors 21\n",
      "microth mentors 19\n",
      "microth mentors 30\n",
      "moschitti mentors 29\n",
      "moschitti mentors 37\n",
      "nivre mentors 10\n",
      "sanda mentors 18\n",
      "tsmoon mentors 9\n",
      "---\n",
      "New best score 27\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 58\n",
      "bonnie mentors 57\n",
      "cbrew mentors 10\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 47\n",
      "dyogatama mentors 29\n",
      "dyogatama mentors 31\n",
      "epitler mentors 39\n",
      "gneubig mentors 19\n",
      "gneubig mentors 27\n",
      "gpenn mentors 38\n",
      "hovy mentors 50\n",
      "karthikn mentors 56\n",
      "ko mentors 24\n",
      "li.jessy mentors 21\n",
      "microth mentors 30\n",
      "moschitti mentors 37\n",
      "nivre mentors 9\n",
      "sanda mentors 18\n",
      "tsmoon mentors 40\n",
      "---\n",
      "New best score 28\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 56\n",
      "bonnie mentors 57\n",
      "cbrew mentors 40\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 19\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 27\n",
      "dyogatama mentors 31\n",
      "epitler mentors 39\n",
      "gneubig mentors 10\n",
      "gpenn mentors 38\n",
      "hovy mentors 47\n",
      "karthikn mentors 58\n",
      "ko mentors 24\n",
      "li.jessy mentors 21\n",
      "microth mentors 30\n",
      "moschitti mentors 29\n",
      "nivre mentors 9\n",
      "sanda mentors 18\n",
      "tsmoon mentors 37\n",
      "tsmoon mentors 50\n",
      "---\n",
      "New best score 29\n",
      "ann.copestake mentors 32\n",
      "bishan mentors 56\n",
      "bonnie mentors 57\n",
      "cbrew mentors 40\n",
      "cdq10131 mentors 60\n",
      "chenhao mentors 19\n",
      "chenhao mentors 52\n",
      "christophteichmann mentors 27\n",
      "dyogatama mentors 31\n",
      "epitler mentors 39\n",
      "gneubig mentors 10\n",
      "gpenn mentors 38\n",
      "hovy mentors 47\n",
      "karthikn mentors 58\n",
      "ko mentors 24\n",
      "li.jessy mentors 21\n",
      "microth mentors 30\n",
      "moschitti mentors 29\n",
      "moschitti mentors 37\n",
      "nivre mentors 9\n",
      "sanda mentors 18\n",
      "tsmoon mentors 50\n",
      "---\n",
      "New best score 30\n",
      "ann.copestake mentors 30\n",
      "bishan mentors 52\n",
      "bonnie mentors 57\n",
      "cbrew mentors 19\n",
      "cbrew mentors 24\n",
      "cdq10131 mentors 58\n",
      "chenhao mentors 21\n",
      "christophteichmann mentors 60\n",
      "dyogatama mentors 31\n",
      "epitler mentors 39\n",
      "gneubig mentors 10\n",
      "gpenn mentors 27\n",
      "hovy mentors 50\n",
      "karthikn mentors 56\n",
      "ko mentors 38\n",
      "li.jessy mentors 29\n",
      "microth mentors 32\n",
      "moschitti mentors 37\n",
      "moschitti mentors 47\n",
      "nivre mentors 9\n",
      "sanda mentors 18\n",
      "tsmoon mentors 40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-40da72db0bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_soln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msoln\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpycosat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitersolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclauses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_soln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'---'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = best_soln = None\n",
    "for soln in pycosat.itersolve(clauses):\n",
    "    score = score_soln(soln)\n",
    "    if best_score is None or score > best_score:\n",
    "        print '---'\n",
    "        print 'New best score', score\n",
    "        best_score = score\n",
    "        best_soln = soln\n",
    "        for cnum in soln:\n",
    "            if cnum <= 0:\n",
    "                continue\n",
    "            ridx, pidx = unclauseid(cnum)\n",
    "            rid = rids[ridx]\n",
    "            pid = pids[pidx]\n",
    "            print rid,'mentors',pid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_soln = [9, 40, 64, 70, 72, 109, 115, 154, 164, 190, 200, 227, 259, 283, 299, 316, 341, 364, 368, 375, 399, 433]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a.zarcone': ['a.zarcone@gmail.com', 'Alessandra', 'Zarcone'],\n",
       " 'ahb': ['antonio.branco@di.fc.ul.pt', 'Ant\\xc3\\xb3nio', 'Branco'],\n",
       " 'angeli': ['angeli@stanford.edu', 'Gabor', 'Angeli'],\n",
       " 'ann.copestake': ['ann.copestake@cl.cam.ac.uk', 'Ann', 'Copestake'],\n",
       " 'avgustinova': ['avgustinova@coli.uni-saarland.de', 'Tania', 'Avgustinova'],\n",
       " 'bdlijiwei': ['jiweil@stanford.edu', 'Jiwei', 'Li'],\n",
       " 'bishan': ['bishan@cs.cmu.edu', 'Bishan', 'Yang'],\n",
       " 'bonnie': ['bonnie.webber@ed.ac.uk', 'Bonnie', 'Webber'],\n",
       " 'cbrew': ['cbrew@acm.org', 'Chris', 'Brew'],\n",
       " 'cdq10131': ['danqi@cs.stanford.edu', 'Danqi', 'Chen'],\n",
       " 'chenhao': ['chenhao@cs.cornell.edu', 'Chenhao', 'Tan'],\n",
       " 'christophteichmann': ['christoph.teichmann@uni-potsdam.de',\n",
       "  'Christoph',\n",
       "  'Teichmann'],\n",
       " 'david.schlangen': ['david.schlangen@uni-bielefeld.de', 'David', 'Schlangen'],\n",
       " 'dilekha': ['dilek@ieee.org', 'Dilek', 'Hakkani-Tur'],\n",
       " 'dyogatama': ['dyogatama@cs.cmu.edu', 'Dani', 'Yogatama'],\n",
       " 'elhadad': ['elhadad@cs.bgu.ac.il', 'Michael', 'Elhadad'],\n",
       " 'epitler': ['emily.pitler@gmail.com', 'Emily', 'Pitler'],\n",
       " 'eric.laporte': ['eric.laporte@univ-paris-est.fr', 'Eric', 'Laporte'],\n",
       " 'foster': ['george.foster@cnrc-nrc.gc.ca', 'George', 'Foster'],\n",
       " 'frcchang': ['yue_zhang@sutd.edu.sg', 'Yue', 'Zhang'],\n",
       " 'gneubig': ['neubig@is.naist.jp', 'Graham', 'Neubig'],\n",
       " 'gpenn': ['gpenn@cs.toronto.edu', 'Gerald', 'Penn'],\n",
       " 'habash': ['nizar.habash@nyu.edu', 'Nizar', 'Habash'],\n",
       " 'hhe': ['hhe@cs.umd.edu', 'He', 'He'],\n",
       " 'hovy': ['hovy@cmu.edu', 'Eduard', 'Hovy'],\n",
       " 'jamesp': ['jamesp@cs.brandeis.edu', 'James', 'Pustejovsky'],\n",
       " 'jiangfeng1124': ['jiangfeng1124@gmail.com', 'Jiang', 'Guo'],\n",
       " 'karthikn': ['karthikn@csail.mit.edu', 'Karthik', 'Narasimhan'],\n",
       " 'kbhall': ['khallbobo@gmail.com', 'Keith', 'Hall'],\n",
       " 'kgimpel': ['kgimpel@ttic.edu', 'Kevin', 'Gimpel'],\n",
       " 'ko': ['ko@cs.cmu.edu', 'Kemal', 'Oflazer'],\n",
       " 'kysu': ['kysu@iis.sinica.edu.tw', 'Keh-Yih', 'Su'],\n",
       " 'larshellan': ['lars.hellan@ntnu.no', 'Lars', 'Hellan'],\n",
       " 'li.jessy': ['ljunyi@seas.upenn.edu', 'Junyi Jessy', 'Li'],\n",
       " 'luwei': ['wei_lu@sutd.edu.sg', 'Wei', 'Lu'],\n",
       " 'marcu': ['marcu@isi.edu', 'Daniel', 'Marcu'],\n",
       " 'masonzms': ['mason.zms@gmail.com', 'Meishan', 'Zhang'],\n",
       " 'microth': ['mroth@inf.ed.ac.uk', 'Michael', 'Roth'],\n",
       " 'miles': ['milesosb@gmail.com', 'Miles', 'Osborne'],\n",
       " 'moschitti': ['amoschitti@gmail.com', 'Alessandro', 'Moschitti'],\n",
       " 'nivre': ['joakim.nivre@lingfil.uu.se', 'Joakim', 'Nivre'],\n",
       " 'npeng': ['pengnanyun@gmail.com', 'Nanyun', 'Peng'],\n",
       " 'pkoehn': ['phi@jhu.edu', 'Philipp', 'Koehn'],\n",
       " 'riley': ['riley@google.com', 'Michael', 'Riley'],\n",
       " 'rws': ['rws@xoba.com', 'Richard', 'Sproat'],\n",
       " 'sanda': ['sanda@hlt.utdallas.edu', 'Sanda', 'Harabagiu'],\n",
       " 'sekine': ['sekine@cs.nyu.edu', 'Satoshi', 'Sekine'],\n",
       " 'simone.teufel': ['simone.teufel@googlemail.com', 'Simone', 'Teufel'],\n",
       " 'taolei': ['taolei@csail.mit.edu', 'Tao', 'Lei'],\n",
       " 'tsmoon': ['taesun.moon@utexas.edu', 'Taesun', 'Moon'],\n",
       " 'vince': ['vince@hlt.utdallas.edu', 'Vincent', 'Ng'],\n",
       " 'wangwilliamyang': ['yww@cs.cmu.edu', 'William Yang', 'Wang'],\n",
       " 'wanxiang': ['wanxiang@gmail.com', 'Wanxiang', 'Che'],\n",
       " 'wroberts': ['will.roberts@anglistik.hu-berlin.de', 'Will', 'Roberts'],\n",
       " 'xiaohe': ['xiaohe@microsoft.com', 'Xiaodong', 'He'],\n",
       " 'yannick.parmentier': ['yannick.parmentier@univ-orleans.fr',\n",
       "  'Yannick',\n",
       "  'Parmentier'],\n",
       " 'yuanzh': ['yuanzh@csail.mit.edu', 'Yuan', 'Zhang'],\n",
       " 'zero12': ['wanglin1122@gmail.com', 'Wang', 'Ling']}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REVIEWERS = '''\n",
    "a.zarcone\ta.zarcone@gmail.com\tAlessandra\tZarcone\n",
    "ahb\tantonio.branco@di.fc.ul.pt\tAntónio\tBranco\n",
    "angeli\tangeli@stanford.edu\tGabor\tAngeli\n",
    "ann.copestake\tann.copestake@cl.cam.ac.uk\tAnn\tCopestake\n",
    "avgustinova\tavgustinova@coli.uni-saarland.de\tTania\tAvgustinova\n",
    "bdlijiwei\tjiweil@stanford.edu\tJiwei\tLi\n",
    "bishan\tbishan@cs.cmu.edu\tBishan\tYang\n",
    "bonnie\tbonnie.webber@ed.ac.uk\tBonnie\tWebber\n",
    "cbrew\tcbrew@acm.org\tChris\tBrew\n",
    "cdq10131\tdanqi@cs.stanford.edu\tDanqi\tChen\n",
    "chenhao\tchenhao@cs.cornell.edu\tChenhao\tTan\n",
    "christophteichmann\tchristoph.teichmann@uni-potsdam.de\tChristoph\tTeichmann\n",
    "david.schlangen\tdavid.schlangen@uni-bielefeld.de\tDavid\tSchlangen\n",
    "dilekha\tdilek@ieee.org\tDilek\tHakkani-Tur\n",
    "dyogatama\tdyogatama@cs.cmu.edu\tDani\tYogatama\n",
    "elhadad\telhadad@cs.bgu.ac.il\tMichael\tElhadad\n",
    "epitler\temily.pitler@gmail.com\tEmily\tPitler\n",
    "eric.laporte\teric.laporte@univ-paris-est.fr\tEric\tLaporte\n",
    "foster\tgeorge.foster@cnrc-nrc.gc.ca\tGeorge\tFoster\n",
    "frcchang\tyue_zhang@sutd.edu.sg\tYue\tZhang\n",
    "gneubig\tneubig@is.naist.jp\tGraham\tNeubig\n",
    "gpenn\tgpenn@cs.toronto.edu\tGerald\tPenn\n",
    "habash\tnizar.habash@nyu.edu\tNizar\tHabash\n",
    "hhe\thhe@cs.umd.edu\tHe\tHe\n",
    "hovy\thovy@cmu.edu\tEduard\tHovy\n",
    "jamesp\tjamesp@cs.brandeis.edu\tJames\tPustejovsky\n",
    "jiangfeng1124\tjiangfeng1124@gmail.com\tJiang\tGuo\n",
    "karthikn\tkarthikn@csail.mit.edu\tKarthik\tNarasimhan\n",
    "kbhall\tkhallbobo@gmail.com\tKeith\tHall\n",
    "kgimpel\tkgimpel@ttic.edu\tKevin\tGimpel\n",
    "ko\tko@cs.cmu.edu\tKemal\tOflazer\n",
    "kysu\tkysu@iis.sinica.edu.tw\tKeh-Yih\tSu\n",
    "larshellan\tlars.hellan@ntnu.no\tLars\tHellan\n",
    "li.jessy\tljunyi@seas.upenn.edu\tJunyi Jessy\tLi\n",
    "luwei\twei_lu@sutd.edu.sg\tWei\tLu\n",
    "marcu\tmarcu@isi.edu\tDaniel\tMarcu\n",
    "masonzms\tmason.zms@gmail.com\tMeishan\tZhang\n",
    "microth\tmroth@inf.ed.ac.uk\tMichael\tRoth\n",
    "miles\tmilesosb@gmail.com\tMiles\tOsborne\n",
    "moschitti\tamoschitti@gmail.com\tAlessandro\tMoschitti\n",
    "nivre\tjoakim.nivre@lingfil.uu.se\tJoakim\tNivre\n",
    "npeng\tpengnanyun@gmail.com\tNanyun\tPeng\n",
    "pkoehn\tphi@jhu.edu\tPhilipp\tKoehn\n",
    "riley\triley@google.com\tMichael\tRiley\n",
    "rws\trws@xoba.com\tRichard\tSproat\n",
    "sanda\tsanda@hlt.utdallas.edu\tSanda\tHarabagiu\n",
    "sekine\tsekine@cs.nyu.edu\tSatoshi\tSekine\n",
    "simone.teufel\tsimone.teufel@googlemail.com\tSimone\tTeufel\n",
    "taolei\ttaolei@csail.mit.edu\tTao\tLei\n",
    "tsmoon\ttaesun.moon@utexas.edu\tTaesun\tMoon\n",
    "vince\tvince@hlt.utdallas.edu\tVincent\tNg\n",
    "wangwilliamyang\tyww@cs.cmu.edu\tWilliam Yang\tWang\n",
    "wanxiang\twanxiang@gmail.com\tWanxiang\tChe\n",
    "wroberts\twill.roberts@anglistik.hu-berlin.de\tWill\tRoberts\n",
    "xiaohe\txiaohe@microsoft.com\tXiaodong\tHe\n",
    "yannick.parmentier\tyannick.parmentier@univ-orleans.fr\tYannick\tParmentier\n",
    "yuanzh\tyuanzh@csail.mit.edu\tYuan\tZhang\n",
    "zero12\twanglin1122@gmail.com\tWang\tLing\n",
    "'''\n",
    "REVIEWERS = [x.split('\\t') for x in REVIEWERS.strip().split('\\n')]\n",
    "REVIEWERS = dict((l[0], l[1:]) for l in REVIEWERS)\n",
    "REVIEWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: [u'Tomonori',\n",
       "  u'Kodaira',\n",
       "  u'sympho-ny@hotmail.co.jp',\n",
       "  u'Controlled and Balanced Dataset for Japanese Lexical Simplification',\n",
       "  u'Accept'],\n",
       " 10: [u'Hitoshi',\n",
       "  u'Otsuki',\n",
       "  u'otsuki@nlp.ist.i.kyoto-u.ac.jp',\n",
       "  u'Dependency Forest based Word Alignment',\n",
       "  u'Accept'],\n",
       " 18: [u'Eric',\n",
       "  u'Benzschawel',\n",
       "  u'ericbenz@brandeis.edu',\n",
       "  u'Identifying Potential Adverse Drug Events in Tweets Using Bootstrapped Lexicons',\n",
       "  u'Accept'],\n",
       " 19: [u'Eri',\n",
       "  u'Matsuo',\n",
       "  u'g1220535@is.ocha.ac.jp',\n",
       "  u'Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity',\n",
       "  u'Accept'],\n",
       " 21: [u'Alron Jan',\n",
       "  u'Lam',\n",
       "  u'alron_lam@dlsu.edu.ph',\n",
       "  u'Improving Twitter Community Detection through Contextual Sentiment Analysis',\n",
       "  u'Accept'],\n",
       " 24: [u'Devadath',\n",
       "  u'V V',\n",
       "  u'devadathv.v@research.iiit.ac.in',\n",
       "  u'Significance of an Accurate Sandhi Splitter in Shallow Parsing of Dravidian languages',\n",
       "  u'Accept'],\n",
       " 27: [u'Clare',\n",
       "  u'Llewellyn',\n",
       "  u'clarellewellyn@yahoo.com',\n",
       "  u'Improving Topic Model Clustering of Newspaper Comments for Summarisation',\n",
       "  u'Accept'],\n",
       " 29: [u'Taha',\n",
       "  u'Tobaili',\n",
       "  u'taha.tobaili@open.ac.uk',\n",
       "  u'Arabizi Detection in Twitter Data',\n",
       "  u'Accept'],\n",
       " 30: [u'Dmitrijs',\n",
       "  u'Milajevs',\n",
       "  u'd.milajevs@qmul.ac.uk',\n",
       "  u'Robust Co-occurrence Quantification for Lexical Distributional Semantics',\n",
       "  u'Accept'],\n",
       " 31: [u'Hessel',\n",
       "  u'Haagsma',\n",
       "  u'hessel.haagsma@rug.nl',\n",
       "  u'Singleton Detection using Word Embeddings and Neural Networks',\n",
       "  u'Accept'],\n",
       " 32: [u'Murhaf',\n",
       "  u'Fares',\n",
       "  u'murhaff@ifi.uio.no',\n",
       "  u'A Dataset for Noun-Noun Compound Bracketing and Interpretation',\n",
       "  u'Accept'],\n",
       " 37: [u'Omid',\n",
       "  u'Moradiannasab',\n",
       "  u'omidmoradiannasab@gmail.com',\n",
       "  u'Thesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents',\n",
       "  u'Accept'],\n",
       " 38: [u'Vincent',\n",
       "  u'Kr\\xed\\u017e',\n",
       "  u'kriz@ufal.mff.cuni.cz',\n",
       "  u'Improving Dependency Parsing Using Sentence Clause Charts',\n",
       "  u'Accept'],\n",
       " 39: [u'Ewa',\n",
       "  u'Muszy\\u0144ska',\n",
       "  u'emm68@cam.ac.uk',\n",
       "  u'Graph- and surface-level sentence chunking',\n",
       "  u'Accept'],\n",
       " 40: [u'Parth',\n",
       "  u'Mehta',\n",
       "  u'parth.mehta126@gmail.com',\n",
       "  u'From Extractive to Abstractive Summarization: A Journey',\n",
       "  u'Accept'],\n",
       " 47: [u'Gavin',\n",
       "  u'Abercrombie',\n",
       "  u'jst662@alumni.ku.dk',\n",
       "  u'Putting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations',\n",
       "  u'Accept'],\n",
       " 50: [u'Alon',\n",
       "  u'Daks',\n",
       "  u'alon.daks@berkeley.edu',\n",
       "  u'Unsupervised Authorial Clustering Based on Syntactic Structure',\n",
       "  u'Accept'],\n",
       " 52: [u'Sapna',\n",
       "  u'Negi',\n",
       "  u'sapna.negi@insight-centre.org',\n",
       "  u'Suggestion Mining from Opinionated Text',\n",
       "  u'Accept'],\n",
       " 56: [u'Yandi',\n",
       "  u'Xia',\n",
       "  u'yxx124730@utdallas.edu',\n",
       "  u'An Efficient Cross-lingual Model for Sentence Classification Without Using Machine Translation',\n",
       "  u'Accept'],\n",
       " 57: [u'Timothy',\n",
       "  u'Lee',\n",
       "  u'leetdh1028@gmail.com',\n",
       "  u'QA-It: Classifying Non-Referential It for Question Answer Pairs',\n",
       "  u'Accept'],\n",
       " 58: [u'Davaajav',\n",
       "  u'Jargalsaikhan',\n",
       "  u'davaajav@ecei.tohoku.ac.jp',\n",
       "  u'Building a Corpus for Japanese Wikification with Fine-Grained Entity Classes',\n",
       "  u'Accept'],\n",
       " 60: [u'Vasu',\n",
       "  u'Jindal',\n",
       "  u'vasu.jindal@utdallas.edu',\n",
       "  u'A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization',\n",
       "  u'Accept']}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTHORS = u'''\n",
    "9\tTomonori\tKodaira\tsympho-ny@hotmail.co.jp\tControlled and Balanced Dataset for Japanese Lexical Simplification\tAccept\n",
    "10\tHitoshi\tOtsuki\totsuki@nlp.ist.i.kyoto-u.ac.jp\tDependency Forest based Word Alignment\tAccept\n",
    "18\tEric\tBenzschawel\tericbenz@brandeis.edu\tIdentifying Potential Adverse Drug Events in Tweets Using Bootstrapped Lexicons\tAccept\n",
    "19\tEri\tMatsuo\tg1220535@is.ocha.ac.jp\tGenerating Natural Language Descriptions for Semantic Representations of Human Brain Activity\tAccept\n",
    "21\tAlron Jan\tLam\talron_lam@dlsu.edu.ph\tImproving Twitter Community Detection through Contextual Sentiment Analysis\tAccept\n",
    "24\tDevadath\tV V\tdevadathv.v@research.iiit.ac.in\tSignificance of an Accurate Sandhi Splitter in Shallow Parsing of Dravidian languages\tAccept\n",
    "27\tClare\tLlewellyn\tclarellewellyn@yahoo.com\tImproving Topic Model Clustering of Newspaper Comments for Summarisation\tAccept\n",
    "29\tTaha\tTobaili\ttaha.tobaili@open.ac.uk\tArabizi Detection in Twitter Data\tAccept\n",
    "30\tDmitrijs\tMilajevs\td.milajevs@qmul.ac.uk\tRobust Co-occurrence Quantification for Lexical Distributional Semantics\tAccept\n",
    "31\tHessel\tHaagsma\thessel.haagsma@rug.nl\tSingleton Detection using Word Embeddings and Neural Networks\tAccept\n",
    "32\tMurhaf\tFares\tmurhaff@ifi.uio.no\tA Dataset for Noun-Noun Compound Bracketing and Interpretation\tAccept\n",
    "37\tOmid\tMoradiannasab\tomidmoradiannasab@gmail.com\tThesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents\tAccept\n",
    "38\tVincent\tKríž\tkriz@ufal.mff.cuni.cz\tImproving Dependency Parsing Using Sentence Clause Charts\tAccept\n",
    "39\tEwa\tMuszyńska\temm68@cam.ac.uk\tGraph- and surface-level sentence chunking\tAccept\n",
    "40\tParth\tMehta\tparth.mehta126@gmail.com\tFrom Extractive to Abstractive Summarization: A Journey\tAccept\n",
    "47\tGavin\tAbercrombie\tjst662@alumni.ku.dk\tPutting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations\tAccept\n",
    "50\tAlon\tDaks\talon.daks@berkeley.edu\tUnsupervised Authorial Clustering Based on Syntactic Structure\tAccept\n",
    "52\tSapna\tNegi\tsapna.negi@insight-centre.org\tSuggestion Mining from Opinionated Text\tAccept\n",
    "56\tYandi\tXia\tyxx124730@utdallas.edu\tAn Efficient Cross-lingual Model for Sentence Classification Without Using Machine Translation\tAccept\n",
    "57\tTimothy\tLee\tleetdh1028@gmail.com\tQA-It: Classifying Non-Referential It for Question Answer Pairs\tAccept\n",
    "58\tDavaajav\tJargalsaikhan\tdavaajav@ecei.tohoku.ac.jp\tBuilding a Corpus for Japanese Wikification with Fine-Grained Entity Classes\tAccept\n",
    "60\tVasu\tJindal\tvasu.jindal@utdallas.edu\tA Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization\tAccept\n",
    "'''\n",
    "AUTHORS = [x.split('\\t') for x in AUTHORS.strip().split('\\n')]\n",
    "AUTHORS = dict((int(l[0]), l[1:]) for l in AUTHORS)\n",
    "AUTHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo \"body1\n",
      "body2\n",
      "body3\" | mail -s \"testing long subject\" -c wildwilhelm@gmail.com will.roberts@anglistik.hu-berlin.de wildwilhelm@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# http://www.developerfiles.com/how-to-send-emails-from-localhost-mac-os-x-el-capitan/\n",
    "# http://apple.stackexchange.com/a/92205\n",
    "# http://postfix.1071664.n5.nabble.com/Can-t-get-Postfix-to-run-td46713.html\n",
    "import os\n",
    "def send_email(body, to = None, subject = None, cc = None, verbose = False, dry_run = False):\n",
    "    if subject is None and cc is None:\n",
    "        raise Exception('no addresses given')\n",
    "    args = ['mail']\n",
    "    if subject is not None:\n",
    "        args.append('-s \"{}\"'.format(subject.replace('\"', '\\\\\"')))\n",
    "    if cc is not None:\n",
    "        if isinstance(cc, list):\n",
    "            args.append('-c {}'.format(','.join(cc)))\n",
    "        else:\n",
    "            args.append('-c {}'.format(cc))\n",
    "    if to is not None:\n",
    "        if isinstance(to, list):\n",
    "            args.append(' '.join(to))\n",
    "        else:\n",
    "            args.append(to)\n",
    "    bodycmd = 'echo \"{}\"'.format(body.replace('\"', '\\\\\"'))\n",
    "    mailcmd = ' '.join(args)\n",
    "    cmdline = '{} | {}'.format(bodycmd, mailcmd)\n",
    "    if verbose:\n",
    "        print cmdline\n",
    "    if not dry_run:\n",
    "        os.system(cmdline)\n",
    "        \n",
    "send_email('body1\\nbody2\\nbody3', \n",
    "           to=['will.roberts@anglistik.hu-berlin.de', 'wildwilhelm@gmail.com'], \n",
    "           subject='testing long subject', \n",
    "           cc='wildwilhelm@gmail.com', \n",
    "           verbose=True, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memail': 'ann.copestake@cl.cam.ac.uk', 'title': u'Robust Co-occurrence Quantification for Lexical Distributional Semantics', 'mentor': 'Ann Copestake', 'student': u'Dmitrijs Milajevs', 'semail': u'd.milajevs@qmul.ac.uk'}\n",
      "echo \"Dear Dmitrijs Milajevs,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Robust Co-occurrence Quantification for Lexical Distributional Semantics\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Ann Copestake.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com d.milajevs@qmul.ac.uk ann.copestake@cl.cam.ac.uk\n",
      "{'memail': 'bishan@cs.cmu.edu', 'title': u'Suggestion Mining from Opinionated Text', 'mentor': 'Bishan Yang', 'student': u'Sapna Negi', 'semail': u'sapna.negi@insight-centre.org'}\n",
      "echo \"Dear Sapna Negi,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Suggestion Mining from Opinionated Text\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Bishan Yang.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com sapna.negi@insight-centre.org bishan@cs.cmu.edu\n",
      "{'memail': 'bonnie.webber@ed.ac.uk', 'title': u'QA-It: Classifying Non-Referential It for Question Answer Pairs', 'mentor': 'Bonnie Webber', 'student': u'Timothy Lee', 'semail': u'leetdh1028@gmail.com'}\n",
      "echo \"Dear Timothy Lee,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"QA-It: Classifying Non-Referential It for Question Answer Pairs\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Bonnie Webber.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com leetdh1028@gmail.com bonnie.webber@ed.ac.uk\n",
      "{'memail': 'cbrew@acm.org', 'title': u'Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity', 'mentor': 'Chris Brew', 'student': u'Eri Matsuo', 'semail': u'g1220535@is.ocha.ac.jp'}\n",
      "echo \"Dear Eri Matsuo,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Chris Brew.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com g1220535@is.ocha.ac.jp cbrew@acm.org\n",
      "{'memail': 'cbrew@acm.org', 'title': u'Significance of an Accurate Sandhi Splitter in Shallow Parsing of Dravidian languages', 'mentor': 'Chris Brew', 'student': u'Devadath V V', 'semail': u'devadathv.v@research.iiit.ac.in'}\n",
      "echo \"Dear Devadath V V,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Significance of an Accurate Sandhi Splitter in Shallow Parsing of Dravidian languages\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Chris Brew.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com devadathv.v@research.iiit.ac.in cbrew@acm.org\n",
      "{'memail': 'danqi@cs.stanford.edu', 'title': u'Building a Corpus for Japanese Wikification with Fine-Grained Entity Classes', 'mentor': 'Danqi Chen', 'student': u'Davaajav Jargalsaikhan', 'semail': u'davaajav@ecei.tohoku.ac.jp'}\n",
      "echo \"Dear Davaajav Jargalsaikhan,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Building a Corpus for Japanese Wikification with Fine-Grained Entity Classes\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Danqi Chen.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com davaajav@ecei.tohoku.ac.jp danqi@cs.stanford.edu\n",
      "{'memail': 'chenhao@cs.cornell.edu', 'title': u'Improving Twitter Community Detection through Contextual Sentiment Analysis', 'mentor': 'Chenhao Tan', 'student': u'Alron Jan Lam', 'semail': u'alron_lam@dlsu.edu.ph'}\n",
      "echo \"Dear Alron Jan Lam,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Improving Twitter Community Detection through Contextual Sentiment Analysis\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Chenhao Tan.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com alron_lam@dlsu.edu.ph chenhao@cs.cornell.edu\n",
      "{'memail': 'christoph.teichmann@uni-potsdam.de', 'title': u'A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization', 'mentor': 'Christoph Teichmann', 'student': u'Vasu Jindal', 'semail': u'vasu.jindal@utdallas.edu'}\n",
      "echo \"Dear Vasu Jindal,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"A Personalized Markov Clustering and Deep Learning Approach for Arabic Text Categorization\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Christoph Teichmann.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com vasu.jindal@utdallas.edu christoph.teichmann@uni-potsdam.de\n",
      "{'memail': 'dyogatama@cs.cmu.edu', 'title': u'Singleton Detection using Word Embeddings and Neural Networks', 'mentor': 'Dani Yogatama', 'student': u'Hessel Haagsma', 'semail': u'hessel.haagsma@rug.nl'}\n",
      "echo \"Dear Hessel Haagsma,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Singleton Detection using Word Embeddings and Neural Networks\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Dani Yogatama.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com hessel.haagsma@rug.nl dyogatama@cs.cmu.edu\n",
      "{'memail': 'emily.pitler@gmail.com', 'title': u'Graph- and surface-level sentence chunking', 'mentor': 'Emily Pitler', 'student': u'Ewa Muszy\\u0144ska', 'semail': u'emm68@cam.ac.uk'}\n",
      "echo \"Dear Ewa Muszyńska,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Graph- and surface-level sentence chunking\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Emily Pitler.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com emm68@cam.ac.uk emily.pitler@gmail.com\n",
      "{'memail': 'neubig@is.naist.jp', 'title': u'Dependency Forest based Word Alignment', 'mentor': 'Graham Neubig', 'student': u'Hitoshi Otsuki', 'semail': u'otsuki@nlp.ist.i.kyoto-u.ac.jp'}\n",
      "echo \"Dear Hitoshi Otsuki,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Dependency Forest based Word Alignment\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Graham Neubig.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com otsuki@nlp.ist.i.kyoto-u.ac.jp neubig@is.naist.jp\n",
      "{'memail': 'gpenn@cs.toronto.edu', 'title': u'Improving Topic Model Clustering of Newspaper Comments for Summarisation', 'mentor': 'Gerald Penn', 'student': u'Clare Llewellyn', 'semail': u'clarellewellyn@yahoo.com'}\n",
      "echo \"Dear Clare Llewellyn,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Improving Topic Model Clustering of Newspaper Comments for Summarisation\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Gerald Penn.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com clarellewellyn@yahoo.com gpenn@cs.toronto.edu\n",
      "{'memail': 'hovy@cmu.edu', 'title': u'Unsupervised Authorial Clustering Based on Syntactic Structure', 'mentor': 'Eduard Hovy', 'student': u'Alon Daks', 'semail': u'alon.daks@berkeley.edu'}\n",
      "echo \"Dear Alon Daks,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Unsupervised Authorial Clustering Based on Syntactic Structure\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Eduard Hovy.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com alon.daks@berkeley.edu hovy@cmu.edu\n",
      "{'memail': 'karthikn@csail.mit.edu', 'title': u'An Efficient Cross-lingual Model for Sentence Classification Without Using Machine Translation', 'mentor': 'Karthik Narasimhan', 'student': u'Yandi Xia', 'semail': u'yxx124730@utdallas.edu'}\n",
      "echo \"Dear Yandi Xia,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"An Efficient Cross-lingual Model for Sentence Classification Without Using Machine Translation\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Karthik Narasimhan.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com yxx124730@utdallas.edu karthikn@csail.mit.edu\n",
      "{'memail': 'ko@cs.cmu.edu', 'title': u'Improving Dependency Parsing Using Sentence Clause Charts', 'mentor': 'Kemal Oflazer', 'student': u'Vincent Kr\\xed\\u017e', 'semail': u'kriz@ufal.mff.cuni.cz'}\n",
      "echo \"Dear Vincent Kríž,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Improving Dependency Parsing Using Sentence Clause Charts\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Kemal Oflazer.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com kriz@ufal.mff.cuni.cz ko@cs.cmu.edu\n",
      "{'memail': 'ljunyi@seas.upenn.edu', 'title': u'Arabizi Detection in Twitter Data', 'mentor': 'Junyi Jessy Li', 'student': u'Taha Tobaili', 'semail': u'taha.tobaili@open.ac.uk'}\n",
      "echo \"Dear Taha Tobaili,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Arabizi Detection in Twitter Data\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Junyi Jessy Li.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com taha.tobaili@open.ac.uk ljunyi@seas.upenn.edu\n",
      "{'memail': 'mroth@inf.ed.ac.uk', 'title': u'A Dataset for Noun-Noun Compound Bracketing and Interpretation', 'mentor': 'Michael Roth', 'student': u'Murhaf Fares', 'semail': u'murhaff@ifi.uio.no'}\n",
      "echo \"Dear Murhaf Fares,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"A Dataset for Noun-Noun Compound Bracketing and Interpretation\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Michael Roth.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com murhaff@ifi.uio.no mroth@inf.ed.ac.uk\n",
      "{'memail': 'amoschitti@gmail.com', 'title': u'Thesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents', 'mentor': 'Alessandro Moschitti', 'student': u'Omid Moradiannasab', 'semail': u'omidmoradiannasab@gmail.com'}\n",
      "echo \"Dear Omid Moradiannasab,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Thesis Proposal: An Investigation on The Effectiveness of Employing Topic Modeling Techniques to Provide Topic Awareness For Conversational Agents\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Alessandro Moschitti.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com omidmoradiannasab@gmail.com amoschitti@gmail.com\n",
      "{'memail': 'amoschitti@gmail.com', 'title': u'Putting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations', 'mentor': 'Alessandro Moschitti', 'student': u'Gavin Abercrombie', 'semail': u'jst662@alumni.ku.dk'}\n",
      "echo \"Dear Gavin Abercrombie,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Putting Sarcasm Detection into Context: The Effects of Class Imbalance and Manual Labelling on Supervised Machine Classification of Twitter Conversations\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Alessandro Moschitti.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com jst662@alumni.ku.dk amoschitti@gmail.com\n",
      "{'memail': 'joakim.nivre@lingfil.uu.se', 'title': u'Controlled and Balanced Dataset for Japanese Lexical Simplification', 'mentor': 'Joakim Nivre', 'student': u'Tomonori Kodaira', 'semail': u'sympho-ny@hotmail.co.jp'}\n",
      "echo \"Dear Tomonori Kodaira,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Controlled and Balanced Dataset for Japanese Lexical Simplification\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Joakim Nivre.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com sympho-ny@hotmail.co.jp joakim.nivre@lingfil.uu.se\n",
      "{'memail': 'sanda@hlt.utdallas.edu', 'title': u'Identifying Potential Adverse Drug Events in Tweets Using Bootstrapped Lexicons', 'mentor': 'Sanda Harabagiu', 'student': u'Eric Benzschawel', 'semail': u'ericbenz@brandeis.edu'}\n",
      "echo \"Dear Eric Benzschawel,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"Identifying Potential Adverse Drug Events in Tweets Using Bootstrapped Lexicons\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Sanda Harabagiu.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com ericbenz@brandeis.edu sanda@hlt.utdallas.edu\n",
      "{'memail': 'taesun.moon@utexas.edu', 'title': u'From Extractive to Abstractive Summarization: A Journey', 'mentor': 'Taesun Moon', 'student': u'Parth Mehta', 'semail': u'parth.mehta126@gmail.com'}\n",
      "echo \"Dear Parth Mehta,\n",
      "\n",
      "Congratulations again on the acceptance of your paper entitled \\\"From Extractive to Abstractive Summarization: A Journey\\\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be Taesun Moon.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
      "\n",
      "Best wishes,\n",
      "He He, Tao Lei, Will Roberts\" | mail -s \"ACL SRW: Mentor assignment for accepted paper\" -c acl-srw-2016@googlegroups.com parth.mehta126@gmail.com taesun.moon@utexas.edu\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE = u'''\n",
    "Dear {{ student }},\n",
    "\n",
    "Congratulations again on the acceptance of your paper entitled \"{{ title }}\".  This email is to put you in contact with your mentor for the 2016 ACL Student Research Workshop, who will be {{ mentor }}.  Your mentor may be able to offer advice on the camera-ready version of your paper; we therefore suggest that you forward them the current version of your article, as well as the reviews which you have received.\n",
    "\n",
    "Best wishes,\n",
    "He He, Tao Lei, Will Roberts\n",
    "'''\n",
    "import jinja2\n",
    "TEMPLATE = jinja2.Template(TEMPLATE.strip())\n",
    "\n",
    "for cnum in best_soln:\n",
    "    if cnum <= 0:\n",
    "        continue\n",
    "    ridx, pidx = unclauseid(cnum)\n",
    "    rid = rids[ridx]\n",
    "    pid = pids[pidx]\n",
    "    tvalues = {}\n",
    "    tvalues['student'] = ' '.join(AUTHORS[pid][:2])\n",
    "    tvalues['semail'] = AUTHORS[pid][2]\n",
    "    tvalues['title'] = AUTHORS[pid][3]\n",
    "    tvalues['memail'] = REVIEWERS[rid][0]\n",
    "    tvalues['mentor'] = ' '.join(REVIEWERS[rid][1:])\n",
    "    print tvalues\n",
    "    #print TEMPLATE.render(tvalues)\n",
    "    send_email(TEMPLATE.render(tvalues).encode('utf-8'), \n",
    "           to=[tvalues['semail'], tvalues['memail']], \n",
    "           subject='ACL SRW: Mentor assignment for accepted paper', \n",
    "           cc='acl-srw-2016@googlegroups.com', \n",
    "           verbose=True) #dry_run=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
